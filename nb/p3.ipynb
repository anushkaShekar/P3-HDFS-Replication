{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42450aa2-e67c-4d28-b1b5-c6d921491d8e",
   "metadata": {},
   "source": [
    "# PART 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb708b27-874d-4262-b55f-76f2a9427cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-21 03:37:00--  https://pages.cs.wisc.edu/~harter/cs639/data/hdma-wi-2021.csv\n",
      "Resolving pages.cs.wisc.edu (pages.cs.wisc.edu)... 128.105.7.9\n",
      "Connecting to pages.cs.wisc.edu (pages.cs.wisc.edu)|128.105.7.9|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 174944099 (167M) [text/csv]\n",
      "Saving to: ‘hdma-wi-2021.csv’\n",
      "\n",
      "hdma-wi-2021.csv    100%[===================>] 166.84M  89.2MB/s    in 1.9s    \n",
      "\n",
      "2023-03-21 03:37:02 (89.2 MB/s) - ‘hdma-wi-2021.csv’ saved [174944099/174944099]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Downloading csv file\n",
    "! wget https://pages.cs.wisc.edu/~harter/cs639/data/hdma-wi-2021.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb582a28-2fe6-46a3-8f0e-d42e88e5b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading same file to HDFS twice\n",
    "! hdfs dfs -D dfs.block.size=1048576 -D dfs.replication=1 -cp hdma-wi-2021.csv hdfs://main:9000/single.csv\n",
    "! hdfs dfs -D dfs.block.size=1048576 -D dfs.replication=2 -cp hdma-wi-2021.csv hdfs://main:9000/double.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4760f897-6dca-414c-850d-e63b407723b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166.8 M  333.7 M  hdfs://main:9000/double.csv\n",
      "166.8 M  166.8 M  hdfs://main:9000/single.csv\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -du -h hdfs://main:9000/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2606cc8-24dc-4693-bbc5-50d6fe894e20",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d3a3b7d-5594-4e2f-95e9-7f1080da6fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59ec451c-8d77-44ea-9471-7944c947441a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'http://e58547bc2648:9864/webhdfs/v1/single.csv': 80,\n",
       " 'http://a0d1575c6b77:9864/webhdfs/v1/single.csv': 87}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks = {}\n",
    "total_blocks = 167\n",
    "datanode1_blocks = 0\n",
    "datanode2_blocks = 0\n",
    "block_size = 1048576\n",
    "offset = 0\n",
    "\n",
    "resp = requests.get(\"http://main:9870/webhdfs/v1/single.csv?op=OPEN&offset={}\".format(offset), allow_redirects=False)\n",
    "location1 = resp.headers[\"Location\"]\n",
    "location1 = location1.split('?')[0]\n",
    "\n",
    "while offset < (total_blocks * block_size):\n",
    "    r = requests.get(\"http://main:9870/webhdfs/v1/single.csv?op=OPEN&offset={}\".format(offset), allow_redirects=False)\n",
    "    location = r.headers[\"Location\"]\n",
    "    location = location.split('?')[0]\n",
    "    \n",
    "    if location1 == location:\n",
    "        datanode1_blocks += 1\n",
    "        blocks[location1] = datanode1_blocks\n",
    "    else:\n",
    "        datanode2_blocks += 1\n",
    "        blocks[location] = datanode2_blocks\n",
    "    offset += block_size\n",
    "blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1ae3d9-0f5a-4493-96a8-17afc253903e",
   "metadata": {},
   "source": [
    "# PART 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8087d13-c6bd-47a4-88f3-74b95adb8605",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 1048576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bfc8b72c-17da-454f-b69a-7152dd4d28fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts from single.csv\n",
      "Single Family: 444874\n",
      "Multi Family: 2493\n",
      "Seconds: 4.200409173965454\n",
      "\n",
      "\n",
      "Counts from single.csv\n",
      "Single Family: 444874\n",
      "Multi Family: 2493\n",
      "Seconds: 7.3432700634002686\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "\n",
    "sf_count = 0\n",
    "mf_count = 0\n",
    "sf2_count = 0\n",
    "mf2_count = 0\n",
    "\n",
    "class hdfsFile(io.RawIOBase):\n",
    "    def __init__(self, path):\n",
    "        resp = requests.get(\"http://main:9870/webhdfs/v1/{}?op=GETFILESTATUS\".format(path))\n",
    "        data = resp.json()\n",
    "        length = data[\"FileStatus\"][\"length\"]\n",
    "        self.path = path\n",
    "        self.offset = 0\n",
    "        self.length = length\n",
    "\n",
    "    def readable(self):\n",
    "        return True\n",
    "\n",
    "    def readinto(self, b):\n",
    "        if self.offset < self.length:\n",
    "            response = requests.get(\"http://main:9870/webhdfs/v1/{}?op=OPEN&offset={}&length={}\".format(self.path, self.offset, len(b)))\n",
    "            if response.status_code != 200:\n",
    "                content = '\\n'\n",
    "                b[:len(content)] = bytes(content, \"utf-8\")\n",
    "                self.offset += BLOCK_SIZE\n",
    "                return 1\n",
    "            else:\n",
    "                txt = response.content\n",
    "                b[:len(txt)] = txt\n",
    "                self.offset += len(txt)\n",
    "                return len(txt)\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "# Buffer Size 1: 1 MB\n",
    "start1 = time.time()\n",
    "for line1 in io.BufferedReader(hdfsFile(\"single.csv\"), buffer_size=1048576):  \n",
    "    line1 = str(line1, \"utf-8\")\n",
    "    if \"Single Family\" in line1:\n",
    "        sf_count += 1\n",
    "    elif \"Multifamily\" in line1:\n",
    "        mf_count += 1\n",
    "end1 = time.time()\n",
    "\n",
    "time1 = end1 - start1\n",
    "\n",
    "print(\"Counts from single.csv\")\n",
    "print(\"Single Family:\", sf_count)\n",
    "print(\"Multi Family:\", mf_count)\n",
    "print(\"Seconds:\", time1)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Buffer Size 2: 1/2 MB\n",
    "start2 = time.time()\n",
    "for line2 in io.BufferedReader(hdfsFile(\"single.csv\"), buffer_size=524288):\n",
    "    line2 = str(line2, \"utf-8\")\n",
    "    if \"Single Family\" in line2:\n",
    "        sf2_count += 1\n",
    "    elif \"Multifamily\" in line2:\n",
    "        mf2_count += 1\n",
    "end2 = time.time()\n",
    "\n",
    "time2 = end2 - start2\n",
    "      \n",
    "print(\"Counts from single.csv\")\n",
    "print(\"Single Family:\", sf2_count)\n",
    "print(\"Multi Family:\", mf2_count)\n",
    "print(\"Seconds:\", time2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb3f882-cab3-41c7-a62e-db2933f3f4a2",
   "metadata": {},
   "source": [
    "# PART 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "34df05a0-214e-43ad-8e48-9d8e5896ce72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 10213466112 (9.51 GB)\n",
      "Present Capacity: 1695715328 (1.58 GB)\n",
      "DFS Remaining: 1433939968 (1.34 GB)\n",
      "DFS Used: 261775360 (249.65 MB)\n",
      "DFS Used%: 15.44%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 167\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 87\n",
      "\tMissing blocks (with replication factor 1): 87\n",
      "\tLow redundancy blocks with highest priority to recover: 167\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (1):\n",
      "\n",
      "Name: 172.18.0.2:9866 (project-3-anushkap3-worker-2.cs544net)\n",
      "Hostname: e58547bc2648\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 10213466112 (9.51 GB)\n",
      "DFS Used: 261775360 (249.65 MB)\n",
      "Non DFS Used: 8500973568 (7.92 GB)\n",
      "DFS Remaining: 1433939968 (1.34 GB)\n",
      "DFS Used%: 2.56%\n",
      "DFS Remaining%: 14.04%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Tue Mar 21 03:44:12 GMT 2023\n",
      "Last Block Report: Tue Mar 21 03:12:06 GMT 2023\n",
      "Num of Blocks: 247\n",
      "\n",
      "\n",
      "Dead datanodes (1):\n",
      "\n",
      "Name: 172.18.0.4:9866 (172.18.0.4)\n",
      "Hostname: a0d1575c6b77\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 10213466112 (9.51 GB)\n",
      "DFS Used: 269313532 (256.84 MB)\n",
      "Non DFS Used: 8493435396 (7.91 GB)\n",
      "DFS Remaining: 1433939968 (1.34 GB)\n",
      "DFS Used%: 2.64%\n",
      "DFS Remaining%: 14.04%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Tue Mar 21 03:41:06 GMT 2023\n",
      "Last Block Report: Tue Mar 21 03:12:06 GMT 2023\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfsadmin -fs hdfs://main:9000/ -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82dafe60-cf68-4777-aa84-d1f8f1e5a253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts from double.csv\n",
      "Single Family: 444874\n",
      "Multi Family: 2493\n",
      "\n",
      "\n",
      "Counts from single.csv\n",
      "Single Family: 212700\n",
      "Multi Family: 1343\n"
     ]
    }
   ],
   "source": [
    "sf3_count = 0\n",
    "mf3_count = 0\n",
    "sf4_count = 0\n",
    "mf4_count = 0\n",
    "\n",
    "# Buffer Size 1: 1 MB - double.csv\n",
    "for line3 in io.BufferedReader(hdfsFile(\"double.csv\"), buffer_size=1048576):\n",
    "    line3 = str(line3, \"utf-8\")\n",
    "    if \"Single Family\" in line3:\n",
    "        sf3_count += 1\n",
    "    elif \"Multifamily\" in line3:\n",
    "        mf3_count += 1\n",
    "\n",
    "print(\"Counts from double.csv\")\n",
    "print(\"Single Family:\", sf3_count)\n",
    "print(\"Multi Family:\", mf3_count)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Buffer Size 1: 1 MB - single.csv \n",
    "for line4 in io.BufferedReader(hdfsFile(\"single.csv\"), buffer_size=1048576):\n",
    "    line4 = str(line4, \"utf-8\")\n",
    "    if \"Single Family\" in line4:\n",
    "        sf4_count += 1\n",
    "    elif \"Multifamily\" in line4:\n",
    "        mf4_count += 1\n",
    "\n",
    "print(\"Counts from single.csv\")\n",
    "print(\"Single Family:\", sf4_count)\n",
    "print(\"Multi Family:\", mf4_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
